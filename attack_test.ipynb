{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyredner # pyredner will be the main Python module we import for redner.\n",
        "import torch # We also import PyTorch\n",
        "from typing import Optional, List\n",
        "import numpy as np\n",
        "from transforms3d import quaternions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def setPos(position, newObj=None):\n",
        "\n",
        "    # position = torch.tensor([0.43, -0.0082,  0.0138]).float()\n",
        "    \n",
        "    # x ~ [0.3, 0.5]\n",
        "    # y ~ [-0.1, 0.1]\n",
        "    # z ~ [0.01, 0.06]\n",
        "    position.requires_grad = True\n",
        "    _x, _y, _z = position\n",
        "    new_vertices = newObj.vertices.clone()\n",
        "    new_vertices = new_vertices * 0.6400000000000001\n",
        "    q = [0.9238634234206463, -0.0007149357294305102, 0.008805899762346357, -0.38262033383206623]\n",
        "    R = quaternions.quat2mat(q)\n",
        "    R = torch.Tensor(R)\n",
        "    new_vertices = torch.matmul(new_vertices, R.T)\n",
        "    new_vertices[:, 0:1] += _x\n",
        "    new_vertices[:, 1:2] += _y\n",
        "    new_vertices[:, 2:3] += _z\n",
        "    newObj.vertices = new_vertices \n",
        "\n",
        "    return position, newObj\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {},
      "outputs": [],
      "source": [
        "def render(obj_list):\n",
        "    \n",
        "    cam_look_at = torch.tensor([0.5, 0.0, 0.0])\n",
        "    cam_position = torch.tensor([0.5, 0.0, 10.0])\n",
        "    camera = pyredner.Camera(position = cam_position,\n",
        "                        look_at = cam_look_at,\n",
        "                        up = torch.tensor([-1.0, 0.0, 0.0]),\n",
        "                        fov = torch.tensor([2.291525676350207]), # in degree\n",
        "                        clip_near = 1e-2, # needs to > 0\n",
        "                        resolution = (128, 128),\n",
        "                        )\n",
        "    scene = pyredner.Scene(camera = camera, objects = obj_list)\n",
        "    chan_list = [pyredner.channels.depth]\n",
        "    depth_img = pyredner.render_generic(scene, chan_list)\n",
        "    # return depth_img.reshape(128,128)\n",
        "    near = 0.09\n",
        "    far = 0.010\n",
        "    depth = near * far /(far - depth_img)\n",
        "    heightmap = torch.abs(depth - torch.max(depth))\n",
        "    heightmap =  heightmap*37821.71428571428 - 3407.3605408838816\n",
        "    heightmap = torch.relu(heightmap)\n",
        "    heightmap = torch.where(heightmap > 1.0, 6e-3, heightmap) \n",
        "    return heightmap.reshape(128,128)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {},
      "outputs": [],
      "source": [
        "# i = render(obj_list)\n",
        "# numpy_array = i.detach().numpy()\n",
        "# file_path = \"_img.txt\"\n",
        "# np.savetxt(file_path, numpy_array)\n",
        "# loss = i.sum()\n",
        "# loss.backward()\n",
        "# p.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from matplotlib.pyplot import imshow\n",
        "# img = i.detach()\n",
        "# imshow(img.cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 定义一个简单的卷积神经网络类\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # 卷积层1\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        # 池化层1\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # 卷积层2\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        # 池化层2\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # 反卷积层1\n",
        "        self.deconv1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        # 反卷积层2\n",
        "        self.deconv2 = nn.ConvTranspose2d(32, 1, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 前向传播\n",
        "        x = self.conv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.deconv1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.deconv2(x)\n",
        "        return x\n",
        "\n",
        "testNet = SimpleCNN()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {},
      "outputs": [],
      "source": [
        "rendering_list = []\n",
        "dir = \"/Users/tingxi/BulletArm/bulletarm/pybullet/urdf/object/GraspNet1B_object/055/convex.obj\"\n",
        "obj = pyredner.load_obj(dir, return_objects=True)\n",
        "newObj = obj[0] \n",
        "epsilon=0.002\n",
        "z_epsilon=epsilon * 1e-04\n",
        "position = torch.tensor([0.43, -0.0082,  0.0138], requires_grad=True).float()\n",
        "xyz_position, newObj = setPos(position, newObj)\n",
        "rendering_list.append(newObj)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {},
      "outputs": [],
      "source": [
        "tray_dir = \"/Users/tingxi/Downloads/Mighty Snaget-Wolt(5)/tinker.obj\"\n",
        "# tray_dir = \"/Users/tingxi/BulletArm/tray.obj\"\n",
        "t = pyredner.load_obj(tray_dir, return_objects=True)\n",
        "tray = t[0]   \n",
        "tray.vertices /= 1000\n",
        "tray.vertices[:,0:1] +=  0.5\n",
        "tray.vertices[:,1:2] += -0.0\n",
        "tray.vertices[:,2:3] += -0.0\n",
        "rendering_list.append(tray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {},
      "outputs": [],
      "source": [
        "xyz_position = xyz_position.clone().detach()\n",
        "rendering_list[0].vertices = rendering_list[0].vertices.clone().detach()\n",
        "ORI_VERTICES = rendering_list[0].vertices.clone().detach()\n",
        "ORI_VERTICES[:,0:1] -= xyz_position[0]\n",
        "ORI_VERTICES[:,1:2] -= xyz_position[1]\n",
        "ORI_VERTICES[:,2:3] -= xyz_position[2]\n",
        "ORI_VERTICES.requires_grad = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "此处把这个object的原始坐标信息单独储存，便于每次渲染生成obs之后重新赋值并detach，用于下一次计算"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter NO.  1\n",
            "tensor([-31931968.0000,  21755560.0000,     88250.4531])\n",
            "gradient:  [tensor(-31931968.), tensor(21755560.), tensor(88250.4531)]\n",
            "OG position:  tensor([ 0.4240, -0.0022,  0.0138], requires_grad=True)\n",
            "eta:  [tensor(-0.0020), tensor(0.0020), tensor(2.0000e-07)]\n",
            "ADV position:  tensor([ 4.2200e-01, -2.0000e-04,  1.3800e-02])\n",
            "iter NO.  2\n",
            "tensor([ 2603257.0000, -2394974.2500,  -102826.0703])\n",
            "gradient:  [tensor(2603257.), tensor(-2394974.2500), tensor(-102826.0703)]\n",
            "OG position:  tensor([ 4.2200e-01, -2.0000e-04,  1.3800e-02], requires_grad=True)\n",
            "eta:  [tensor(0.0020), tensor(-0.0020), tensor(-2.0000e-07)]\n",
            "ADV position:  tensor([ 0.4240, -0.0022,  0.0138])\n",
            "iter NO.  3\n",
            "tensor([-1.5615e+07,  2.1225e+07,  1.8609e+04])\n",
            "gradient:  [tensor(-15615426.), tensor(21225498.), tensor(18608.6895)]\n",
            "OG position:  tensor([ 0.4240, -0.0022,  0.0138], requires_grad=True)\n",
            "eta:  [tensor(-0.0020), tensor(0.0020), tensor(2.0000e-07)]\n",
            "ADV position:  tensor([ 4.2200e-01, -2.0000e-04,  1.3800e-02])\n"
          ]
        }
      ],
      "source": [
        "for iter in range(3):\n",
        "    print(\"iter NO. \", iter+1)\n",
        "    xyz_position.requires_grad = True\n",
        "    new_vertices = ORI_VERTICES.clone()\n",
        "    new_vertices[:,0:1] += xyz_position[0]\n",
        "    new_vertices[:,1:2] += xyz_position[1]\n",
        "    new_vertices[:,2:3] += xyz_position[2]\n",
        "    rendering_list[0].vertices = new_vertices.clone()\n",
        "\n",
        "    obs = render(rendering_list)\n",
        "    obs = obs.reshape(1,1,128,128)\n",
        "    q_value_maps = testNet(obs)\n",
        "    loss = q_value_maps.sum()\n",
        "    loss.backward()\n",
        "    print(xyz_position.grad)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        x_grad, y_grad, z_grad = xyz_position.grad.clone()\n",
        "        xyz_position.grad.zero_()\n",
        "        x,y,z = xyz_position.clone().detach()\n",
        "        # step length should be within a certain range\n",
        "        x_eta = torch.clamp(x_grad.sign(), min = -epsilon,   max = epsilon).detach()\n",
        "        y_eta = torch.clamp(y_grad.sign(), min = -epsilon,   max = epsilon).detach()\n",
        "        z_eta = torch.clamp(z_grad.sign(), min = -z_epsilon, max = z_epsilon).detach()\n",
        "        # coordinate boudary of the object, please do not change these values\n",
        "        # valid range of x and y is 0.2 while for z the range is 0.000025\n",
        "        # accumulated change should not exceed the boundaries\n",
        "        adv_position = torch.tensor([\n",
        "            torch.clamp(x + x_eta, min =  0.4, max = 0.6).detach(),\n",
        "            torch.clamp(y + y_eta, min = -0.1, max = 0.1).detach(),\n",
        "            torch.clamp(z + z_eta, min =  0.013800, max = 0.013825).detach()\n",
        "        ])\n",
        "    print(\"gradient: \", [x_grad, y_grad, z_grad])\n",
        "    print(\"OG position: \", xyz_position)\n",
        "    print(\"eta: \", [x_eta, y_eta, z_eta])\n",
        "    print(\"ADV position: \", adv_position)\n",
        "    xyz_position = adv_position.clone().detach()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {},
      "outputs": [],
      "source": [
        "t1 = torch.tensor([1.], requires_grad=True)\n",
        "t2 = torch.tensor([2.], requires_grad=False)\n",
        "m = t1**2\n",
        "l = m.sum()\n",
        "l.backward()\n",
        "t1 = t2\n",
        "t1.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test_1 = SimpleCNN()\n",
        "# test_2 = SimpleCNN()\n",
        "# rendering_list_2 = []\n",
        "# dir_2 = \"/Users/tingxi/BulletArm/bulletarm/pybullet/urdf/object/GraspNet1B_object/055/convex.obj\"\n",
        "# obj_2 = pyredner.load_obj(dir_2, return_objects=True)\n",
        "# newObj_2 = obj_2[0] \n",
        "# position_2 = torch.tensor([0.43, -0.0082,  0.0138], requires_grad=True).float()\n",
        "# xyz_position_2, newObj_2 = setPos(position_2, newObj_2)\n",
        "# rendering_list_2.append(newObj_2)\n",
        "# obs_2 = render(obj_list=rendering_list_2)\n",
        "\n",
        "# for iter in range(3):\n",
        "#     print(\"iter NO. \", iter+1)\n",
        "#     q_value_maps_2 = test_1(obs_2)\n",
        "#     loss_2 = q_value_maps_2.sum()\n",
        "#     loss_2.backward()\n",
        "#     testNet.zero_grad()\n",
        "#     print(xyz_position_2.grad)\n",
        "#     xyz_position_2 = xyz_position_2.detach() + torch.randn(1,1,1,1)\n",
        "#     xyz_position_2.requires_grad = True\n",
        "#     obs_2 = test_2(xyz_position_2)\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "hello_redner.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bullet37",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "256d2f8aa2bc1579d1e3c2d13d7a01ca72bfb833eb287453af22bbc377a06d4d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
