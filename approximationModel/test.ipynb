{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "file_path = '../sample_file/q_map_0.txt'\n",
    "matrix = np.loadtxt(file_path)\n",
    "matrix = matrix.reshape(1,1,128,128)\n",
    "matrix = torch.tensor(matrix)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax2d(tensor):\n",
    "  n = tensor.size(0)\n",
    "  d = tensor.size(2)\n",
    "  m = tensor.view(n, -1).argmax(1)\n",
    "  return torch.cat(((m / d).view(-1, 1), (m % d).view(-1, 1)), dim=1)\n",
    "\n",
    "argmax2d(matrix)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_matrix = matrix.clone()\n",
    "for i in range(100):\n",
    "    max_index = argmax2d(copy_matrix)[0].long()\n",
    "    print(max_index, copy_matrix[0][0][max_index[0]][max_index[1]])\n",
    "    copy_matrix[0][0][max_index[0]][max_index[1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def argmax2d(tensor):\n",
    "    n = tensor.size(0)\n",
    "    d = tensor.size(2)\n",
    "    m = tensor.view(n, -1).argmax(1)\n",
    "    return torch.cat(((m / d).view(-1, 1), (m % d).view(-1, 1)), dim=1)\n",
    "\n",
    "def set_center_region(submatrix, n, value):\n",
    "    # 获取中心区域的边界\n",
    "    min_row = submatrix.shape[0] // 2 - n // 2\n",
    "    max_row = min_row + n\n",
    "    min_col = submatrix.shape[1] // 2 - n // 2\n",
    "    max_col = min_col + n\n",
    "\n",
    "    # 将中心区域的值设置为指定值\n",
    "    submatrix[min_row:max_row, min_col:max_col] = value\n",
    "    return submatrix\n",
    "\n",
    "\n",
    "def find_second_max(matrix, center, neighborhood_size=20, min_distance=10):\n",
    "\n",
    "    a, b = center\n",
    "\n",
    "    # 定义邻域的边界\n",
    "    min_row = max(0, a - neighborhood_size // 2)\n",
    "    max_row = min(matrix.shape[0], a + neighborhood_size // 2 + 1)\n",
    "    min_col = max(0, b - neighborhood_size // 2)\n",
    "    max_col = min(matrix.shape[1], b + neighborhood_size // 2 + 1)\n",
    "\n",
    "    # 在邻域内找到次最大值的坐标\n",
    "    submatrix = matrix[min_row:max_row, min_col:max_col]\n",
    "    submatrix = set_center_region(submatrix, min_distance, 0)\n",
    "    flattened_indices = np.argsort(submatrix, axis=None)  # 将邻域展平后的索引\n",
    "    second_max_index = np.unravel_index(flattened_indices[-2], submatrix.shape)\n",
    "\n",
    "    # 转换为全局坐标\n",
    "    second_max_coord = (a - neighborhood_size // 2 + second_max_index[0],\n",
    "                        b - neighborhood_size // 2 + second_max_index[1])\n",
    "\n",
    "    return second_max_coord\n",
    "\n",
    "file_path = '../sample_file/q_map_1.txt'\n",
    "matrix = np.loadtxt(file_path)\n",
    "matrix = torch.tensor(matrix)\n",
    "\n",
    "# 找到最大值的坐标\n",
    "max_coords = argmax2d(matrix.reshape(1,1,128,128)).long()[0]\n",
    "\n",
    "# 找到次最大值的坐标\n",
    "second_max_coords = find_second_max(matrix, max_coords)\n",
    "\n",
    "print(\"最大值坐标:\", max_coords)\n",
    "print(\"次最大值坐标:\", second_max_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义一个字典来存储每个 action 的累计 reward 和计数器\n",
    "action_rewards = {}\n",
    "\n",
    "# 打开文件并逐行读取\n",
    "with open(\"../temp/object_attacked_position.txt\") as file:\n",
    "    for line in file:\n",
    "        # 解析每一行，提取出 action 和对应的 reward\n",
    "        values = [float(match) for match in re.findall(r'[-+]?\\d*\\.\\d+(?:[eE][-+]?\\d+)?|\\d+', line)]\n",
    "        action = tuple(values[7:11])  # 提取 action，这里假设 action 是一个由四个元素组成的元组\n",
    "        reward = values[-1]  # 提取 reward\n",
    "\n",
    "        # 更新字典中对应 action 的累计 reward 和计数器\n",
    "        if action in action_rewards:\n",
    "            action_rewards[action]['total_reward'] += reward\n",
    "            action_rewards[action]['count'] += 1\n",
    "        else:\n",
    "            action_rewards[action] = {'total_reward': reward, 'count': 1}\n",
    "\n",
    "# 计算每个 action 的平均 reward\n",
    "average_rewards = {}\n",
    "for action, reward_info in action_rewards.items():\n",
    "    average_reward = reward_info['total_reward'] / reward_info['count']\n",
    "    average_rewards[action] = average_reward\n",
    "\n",
    "# 提取每个动作的x和y坐标\n",
    "x_coords = [action[0] for action in average_rewards.keys()]\n",
    "y_coords = [action[1] for action in average_rewards.keys()]\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.scatter(x_coords, y_coords, s=30, c=list(average_rewards.values()), cmap='viridis')\n",
    "plt.colorbar(label='Average Reward')\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Average Reward for Each Action')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 定义一个字典来存储每个 action 的累计 reward 和计数器\n",
    "action_rewards = {}\n",
    "\n",
    "# 打开文件并逐行读取\n",
    "with open(\"../temp/object_attacked_position.txt\") as file:\n",
    "    for line in file:\n",
    "        # 解析每一行，提取出 action 和对应的 reward\n",
    "        values = [float(match) for match in re.findall(r'[-+]?\\d*\\.\\d+(?:[eE][-+]?\\d+)?|\\d+', line)]\n",
    "        action = tuple(values[7:11])  # 提取 action，这里假设 action 是一个由四个元素组成的元组\n",
    "        reward = values[-1]  # 提取 reward\n",
    "\n",
    "        # 更新字典中对应 action 的累计 reward 和计数器\n",
    "        if action in action_rewards:\n",
    "            action_rewards[action]['total_reward'] += reward\n",
    "            action_rewards[action]['count'] += 1\n",
    "        else:\n",
    "            action_rewards[action] = {'total_reward': reward, 'count': 1}\n",
    "\n",
    "# 计算每个 action 的平均 reward\n",
    "average_rewards = {}\n",
    "for action, reward_info in action_rewards.items():\n",
    "    average_reward = reward_info['total_reward'] / reward_info['count']\n",
    "    average_rewards[action] = average_reward\n",
    "\n",
    "# 提取每个动作的平均奖励值\n",
    "average_rewards_values = list(average_rewards.values())\n",
    "\n",
    "# 划分平均奖励区间\n",
    "num_bins = 10\n",
    "bins = np.linspace(min(average_rewards_values), max(average_rewards_values), num_bins + 1)\n",
    "\n",
    "# 统计每个区间内动作的数量\n",
    "action_counts = {i: 0 for i in range(num_bins)}\n",
    "for reward in average_rewards_values:\n",
    "    for i in range(num_bins):\n",
    "        if bins[i] <= reward < bins[i+1]:\n",
    "            action_counts[i] += 1\n",
    "            break\n",
    "\n",
    "# 绘制柱状图\n",
    "plt.bar(range(num_bins), action_counts.values(), align='center', alpha=0.5)\n",
    "plt.xticks(range(num_bins), [f\"{bins[i]:.2f}-{bins[i+1]:.2f}\" for i in range(num_bins)])\n",
    "plt.xlabel('Average Reward Range')\n",
    "plt.ylabel('Number of Actions')\n",
    "plt.title('Number of Actions in Each Average Reward Range')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(4,4)\n",
    "torch.argmax(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: no CUDA device found, running on CPU\n",
      "torch.Size([1, 1, 128, 128]) torch.Size([1, 16]) torch.Size([1, 3]) torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensor_paths, device):\n",
    "        self.tensor_paths = tensor_paths\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tensor_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tensor_path = self.tensor_paths[idx]\n",
    "        q_value_map, q2_output, action, reward = load_tensor(tensor_path, self.device)\n",
    "        return q_value_map, q2_output, action, reward\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input1_channels, input2_size, input3_size, hidden_size, output_size):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        # Input1的处理\n",
    "        self.conv1 = nn.Conv2d(input1_channels, 4, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.maxpooling = nn.MaxPool2d(kernel_size=4)\n",
    "        self.fc1 = nn.Linear(16384, 128)\n",
    "        self.fc2 = nn.Linear(128, hidden_size)\n",
    "\n",
    "        # Input2的处理\n",
    "        self.fc3 = nn.Linear(input2_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Input3的处理\n",
    "        self.fc5 = nn.Linear(input3_size, hidden_size)\n",
    "        self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(hidden_size * 3, output_size)\n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        # Input1的处理\n",
    "        print(input1.shape)\n",
    "        x1 = torch.relu(self.conv1(input1))\n",
    "        x1 = torch.relu(self.conv2(x1))\n",
    "        x1 = torch.relu(self.conv3(x1))\n",
    "        x1 = self.maxpooling(x1)\n",
    "        x1 = torch.flatten(x1, 1)\n",
    "        print(x1.shape)\n",
    "        x1 = torch.relu(self.fc1(x1))\n",
    "        x1 = torch.relu(self.fc2(x1))\n",
    "\n",
    "        # Input2的处理\n",
    "        x2 = torch.relu(self.fc3(input2))\n",
    "        x2 = torch.relu(self.fc4(x2))\n",
    "\n",
    "        # Input3的处理\n",
    "        x3 = torch.relu(self.fc5(input3))\n",
    "        x3 = torch.relu(self.fc6(x3))\n",
    "\n",
    "        # 将三个输入连接起来\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "\n",
    "        # 输出层\n",
    "        output = self.output_layer(x)\n",
    "\n",
    "        return output\n",
    "    \n",
    "def generate_path_list():\n",
    "    path_list = [f\"../dataset/{i}_{j}.pt\" for i in range(86) for j in range(100)]\n",
    "    return path_list\n",
    "\n",
    "def load_tensor(tensor_path, device):\n",
    "    tensor = torch.load(tensor_path, map_location=device).detach()\n",
    "    q_value_map = tensor[:128*128].reshape(1,128,128)\n",
    "    q2_output = tensor[128*128:-4]\n",
    "    action = tensor[-4:-1]\n",
    "    reward = tensor[-1:]\n",
    "    return q_value_map, q2_output, action, reward\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "set_random_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.device('cuda')\n",
    "    print(\"found CUDA device\")\n",
    "else:\n",
    "    # raise ValueError(\"cannot run on cpu device\")\n",
    "    device_name = torch.device('cpu')\n",
    "    print(\"Warning: no CUDA device found, running on CPU\")\n",
    "\n",
    "data_path_list = generate_path_list()\n",
    "data_num = len(data_path_list)\n",
    "random.shuffle(data_path_list)\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(data_path_list))\n",
    "eval_size = int(0.1 * len(data_path_list))\n",
    "test_size = len(data_path_list) - train_size - eval_size\n",
    "\n",
    "train_dataset = CustomDataset(data_path_list[:train_size], device_name)\n",
    "eval_dataset = CustomDataset(data_path_list[train_size:train_size+eval_size], device_name)\n",
    "test_dataset = CustomDataset(data_path_list[train_size+eval_size:], device_name)\n",
    "\n",
    "input1_channels = 1\n",
    "input2_size = 16\n",
    "input3_size = 3\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "MyModel = CustomModel(input1_channels, input2_size, input3_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(MyModel.parameters(), lr=0.001)\n",
    "\n",
    "for a,b,c,d in train_loader:\n",
    "    print(a.shape,b.shape,c.shape,d.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/25, Training:   0%|          | 0/6880 [01:09<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128])\n",
      "torch.Size([1, 16384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Evaluating:   0%|          | 0/860 [01:09<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128])\n",
      "torch.Size([1, 16384])\n",
      "Epoch 1/25, Average Train Loss: 0.0001, Average Eval Loss: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "log_train_loss = []\n",
    "log_eval_loss = []\n",
    "eval_interval = 5\n",
    "for e in range(1):\n",
    "    \n",
    "    MyModel.train()\n",
    "    train_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f'Epoch {e+1}/{num_epochs}, Training')\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        q_value_map, q2_output, action, reward = data\n",
    "        output = MyModel(q_value_map, q2_output, action)\n",
    "        loss = criterion(output, reward)\n",
    "        log_train_loss.append(loss.detach().item())\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        break\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    \n",
    "    MyModel.eval()\n",
    "    eval_loss = 0.0\n",
    "    eval_bar = tqdm(eval_loader, desc=f'Epoch {e+1}/{num_epochs}, Evaluating')\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(eval_loader):\n",
    "            q_value_map, q2_output, action, reward = data\n",
    "            output = MyModel(q_value_map, q2_output, action)\n",
    "            loss = criterion(output, reward)\n",
    "            log_eval_loss.append(loss.detach().item())\n",
    "            eval_loss += loss.item()\n",
    "            break\n",
    "\n",
    "    avg_eval_loss = eval_loss / len(eval_loader.dataset)\n",
    "    \n",
    "    print(f'Epoch {e+1}/{num_epochs}, Average Train Loss: {avg_train_loss:.4f}, Average Eval Loss: {avg_eval_loss:.4f}')\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14333553612232208]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomModel(\n",
      "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpooling): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=16384, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=3, out_features=64, bias=True)\n",
      "  (fc6): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (output_layer): Linear(in_features=192, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training:   0%|          | 0/6880 [00:45<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x1024 and 16384x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[150], line 156\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    155\u001b[0m     q_value_map, q2_output, action, reward \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m--> 156\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mMyModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_value_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq2_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, reward)\n\u001b[1;32m    159\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bullet38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[150], line 74\u001b[0m, in \u001b[0;36mCustomModel.forward\u001b[0;34m(self, input1, input2, input3)\u001b[0m\n\u001b[1;32m     72\u001b[0m x1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x1, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(x1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 74\u001b[0m x1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     75\u001b[0m x1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x1))\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Input2的处理\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bullet38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bullet38/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x1024 and 16384x128)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensor_paths, device):\n",
    "        self.tensor_paths = tensor_paths\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tensor_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tensor_path = self.tensor_paths[idx]\n",
    "        q_value_map, q2_output, action, reward = load_tensor(tensor_path, self.device)\n",
    "        return q_value_map, q2_output, action, reward\n",
    "    \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensor_paths, device):\n",
    "        self.tensor_paths = tensor_paths\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tensor_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tensor_path = self.tensor_paths[idx]\n",
    "        q_value_map, q2_output, action, reward = load_tensor(tensor_path, self.device)\n",
    "        return q_value_map, q2_output, action, reward\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input1_channels, input2_size, input3_size, hidden_size, output_size):\n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        # Input1的处理\n",
    "        self.conv1 = nn.Conv2d(input1_channels, 4, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.maxpooling = nn.MaxPool2d(kernel_size=4)\n",
    "        self.fc1 = nn.Linear(16384, 128)\n",
    "        self.fc2 = nn.Linear(128, hidden_size)\n",
    "\n",
    "        # Input2的处理\n",
    "        self.fc3 = nn.Linear(input2_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Input3的处理\n",
    "        self.fc5 = nn.Linear(input3_size, hidden_size)\n",
    "        self.fc6 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(hidden_size * 3, output_size)\n",
    "\n",
    "    def forward(self, input1, input2, input3):\n",
    "        # Input1的处理\n",
    "        x1 = torch.relu(self.conv1(input1))\n",
    "        x1 = torch.relu(self.conv2(x1))\n",
    "        x1 = torch.relu(self.conv3(x1))\n",
    "        x1 = self.maxpooling(x1)\n",
    "        x1 = torch.flatten(x1, 1)\n",
    "\n",
    "        x1 = torch.relu(self.fc1(x1))\n",
    "        x1 = torch.relu(self.fc2(x1))\n",
    "\n",
    "        # Input2的处理\n",
    "        x2 = torch.relu(self.fc3(input2))\n",
    "        x2 = torch.relu(self.fc4(x2))\n",
    "\n",
    "        # Input3的处理\n",
    "        x3 = torch.relu(self.fc5(input3))\n",
    "        x3 = torch.relu(self.fc6(x3))\n",
    "\n",
    "        # 将三个输入连接起来\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "\n",
    "        # 输出层\n",
    "        output = self.output_layer(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "def generate_path_list():\n",
    "    path_list = [f\"../dataset/{i}_{j}.pt\" for i in range(86) for j in range(100)]\n",
    "    return path_list\n",
    "\n",
    "def load_tensor(tensor_path, device):\n",
    "    tensor = torch.load(tensor_path, map_location=device).detach()\n",
    "    q_value_map = tensor[:128*128].reshape(1,128,128)\n",
    "    q2_output = tensor[128*128:-4]\n",
    "    action = tensor[-4:-1]\n",
    "    reward = tensor[-1:]\n",
    "    return q_value_map, q2_output, action, reward\n",
    "\n",
    "\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "data_path_list = generate_path_list()\n",
    "data_num = len(data_path_list)\n",
    "random.shuffle(data_path_list)\n",
    "\n",
    "input1_channels = 1\n",
    "input2_size = 16\n",
    "input3_size = 3\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "num_epochs = 100\n",
    "batch_size = 1\n",
    "log_train_loss = []\n",
    "log_eval_loss = []\n",
    "log_test_loss = []\n",
    "best_eval_loss = float('inf')\n",
    "\n",
    "MyModel = CustomModel(input1_channels, input2_size, input3_size, hidden_size, output_size)\n",
    "print(MyModel)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(MyModel.parameters(), lr=0.001)\n",
    "\n",
    "train_size = int(0.8 * len(data_path_list))\n",
    "eval_size = int(0.1 * len(data_path_list))\n",
    "test_size = len(data_path_list) - train_size - eval_size\n",
    "\n",
    "train_dataset = CustomDataset(data_path_list[:train_size], device_name)\n",
    "eval_dataset = CustomDataset(data_path_list[train_size:train_size+eval_size], device_name)\n",
    "test_dataset = CustomDataset(data_path_list[train_size+eval_size:], device_name)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    \n",
    "    MyModel.train()\n",
    "    train_loss = 0\n",
    "    train_bar = tqdm(train_loader, desc=f'Epoch {e+1}/{num_epochs}, Training')\n",
    "    for idx, data in enumerate(train_loader):\n",
    "        q_value_map, q2_output, action, reward = data\n",
    "        output = MyModel(q_value_map, q2_output, action)\n",
    "        loss = criterion(output, reward)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    log_train_loss.append(avg_train_loss)\n",
    "    \n",
    "    MyModel.eval()\n",
    "    eval_loss = 0\n",
    "    eval_bar = tqdm(eval_loader, desc=f'Epoch {e+1}/{num_epochs}, Evaluating')\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(eval_loader):\n",
    "            q_value_map, q2_output, action, reward = data\n",
    "            output = MyModel(q_value_map, q2_output, action)\n",
    "            loss = criterion(output, reward)\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    avg_eval_loss = eval_loss / len(eval_loader.dataset)\n",
    "    log_eval_loss.append(avg_eval_loss)\n",
    "    \n",
    "    print(f'Epoch {e+1}/{num_epochs}, Average Train Loss: {avg_train_loss:.4f}, Average Eval Loss: {avg_eval_loss:.4f}')\n",
    "    \n",
    "    if avg_eval_loss < best_eval_loss:\n",
    "        best_eval_loss = avg_eval_loss\n",
    "        torch.save(MyModel.state_dict(), 'best_model_checkpoint.pth')\n",
    "    \n",
    "MyModel.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(test_loader):\n",
    "        q_value_map, q2_output, action, reward = data\n",
    "        output = MyModel(q_value_map, q2_output, action)\n",
    "        loss = criterion(output, reward)\n",
    "        log_test_loss.append(loss.detach().item())\n",
    "        eval_loss += loss.item()\n",
    "avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "print(f'Average Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "plt.plot(log_train_loss, label='Train Loss')\n",
    "plt.plot(log_eval_loss, label='Eval Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training, Evaluation Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss_curve.png')\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(log_test_loss, label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Testing Loss')\n",
    "plt.savefig('test_loss_figure.png')\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "print(\"=== Task Done === \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bullet37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
